import os
import re
import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple

import yaml
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
from tqdm import tqdm

from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from beats.BEATs import BEATs, BEATsConfig


# ---------------------------
# Parameters
# ---------------------------
def parse_args():
    ap = argparse.ArgumentParser(description="Diagnoser end-to-end pipeline")
    ap.add_argument("--audio_dir", type=str,
                    default="/mnt/data/RespAgent/Agent/Diagnoser/example/audio",
                    help="Directory of wav files to be processed")
    ap.add_argument("--output_dir", type=str,
                    default="/mnt/data/RespAgent/Agent/Diagnoser/output_diagnose",
                    help="Directory to save annotation files and results")
    ap.add_argument("--metadata_csv", type=str,
                    default="/mnt/data/RespAgent/Agent/Diagnoser/example/combined_metadata.csv",
                    help="Path to the combined_metadata.csv of UK_COVID_19")
    ap.add_argument("--config_yaml", type=str,
                    default="/mnt/data/RespAgent/Agent/Diagnoser/config.yaml",
                    help="config.yaml used during training (for reading sample_rate/length etc.)")
    ap.add_argument("--ckpt_dir", type=str,
                    default="/mnt/data/RespAgent/Agent/Diagnoser/checkpoints/longformer",
                    help="Directory of the best Longformer checkpoint (searches for best_longformer_loss_*.pth)")
    ap.add_argument("--thinker_model_path", type=str,
                    default="/mnt/data/RespAgent/Agent/Diagnoser/checkpoints/deepseek-r1",
                    help="Path (directory) to the local LLM for generating descriptions")
    ap.add_argument("--device", type=str, default="cuda:0",
                    help="Default is cuda:0")
    ap.add_argument("--batch_size", type=int, default=2,
                    help="Batch size for LLM description generation")
    ap.add_argument("--max_new_tokens", type=int, default=1024,
                    help="Maximum new tokens to be generated by LLM")
    ap.add_argument("--temperature", type=float, default=0.6)
    ap.add_argument("--top_p", type=float, default=0.9)
    return ap.parse_args()


# ---------------------------
# Utilities: Load/Align Audio
# ---------------------------
def safe_load_audio(path: str):
    """Fault-tolerant loading as a mono channel [1, T]"""
    try:
        if (not os.path.exists(path)) or os.path.getsize(path) == 0:
            return None, 0
        waveform, sr = torchaudio.load(path)
        if waveform.numel() == 0:
            return None, 0
        if waveform.dim() > 1 and waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        if waveform.dim() == 1:
            waveform = waveform.unsqueeze(0)
        return waveform, sr
    except Exception:
        return None, 0


def resample_pad_or_trim(waveform: torch.Tensor, sr: int, target_sr: int, target_len_samples: int):
    """Resample to target_sr and trim/pad to a fixed length"""
    if sr != target_sr:
        try:
            waveform = torchaudio.transforms.Resample(sr, target_sr)(waveform)
        except Exception:
            return None
    cur = waveform.shape[1]
    if cur > target_len_samples:
        waveform = waveform[:, :target_len_samples]
    else:
        waveform = torch.nn.functional.pad(waveform, (0, target_len_samples - cur))
    return waveform


def build_prompt(row: pd.Series, audio_filename: str, audio_type: str) -> str:
    """English summary prompt template"""
    # Patient info
    patient_info = f"""- Patient ID: {row['participant_identifier']}
- Age: {row['age']}
- Gender: {row['gender']}
- Region: {row['region_name']}
- Smoker Status: {row['smoker_status']}
- History of Asthma: {'Yes' if row['respiratory_condition_asthma'] == 1 else 'No'}
- Other Respiratory Conditions: {'Yes' if row['respiratory_condition_other'] == 1 else 'No'}"""

    # Symptoms
    symptoms = []
    if row.get('symptom_cough_any', 0) == 1: symptoms.append('Any cough')
    if row.get('symptom_new_continuous_cough', 0) == 1: symptoms.append('New continuous cough')
    if row.get('symptom_shortness_of_breath', 0) == 1: symptoms.append('Shortness of breath')
    if row.get('symptom_sore_throat', 0) == 1: symptoms.append('Sore throat')
    if row.get('symptom_fatigue', 0) == 1: symptoms.append('Fatigue')
    if row.get('symptom_fever_high_temperature', 0) == 1: symptoms.append('Fever')
    symptoms_str = ', '.join(symptoms) if symptoms else 'No reported symptoms'

    covid_info = f"""- COVID Viral Load Category: {row.get('covid_viral_load_category', 'Unknown')}
- Reported Symptoms: {symptoms_str}"""

    audio_info = f"""- Audio Type: {audio_type}"""

    return f"""Based on the following information, please generate a professional, fluent, and natural English summary description for the audio file '{audio_filename}'. Output the summary description directly, without any explanation or thought process.

### Patient Information
{patient_info}

### COVID-19 Related Information
{covid_info}

### Audio File Information
{audio_info}

### Summary Description:
"""


def clean_llm_output(response_text: str) -> str:
    """Post-processing and cleaning of the generated output"""
    import re as _re
    text_to_clean = response_text.split("</think>")[-1] if "</think>" in response_text else response_text
    if "### Summary Description:" in text_to_clean:
        text_to_clean = text_to_clean.split("### Summary Description:")[-1]
    cleaned_text = _re.sub(r"^\s*Here is the summary.*\s*[:ï¼š]\s*", "", text_to_clean, flags=_re.IGNORECASE)
    return cleaned_text.strip()


# ---------------------------
# Step 1: Generate descriptions for wav files in the specified directory (write to audio_descriptions.jsonl)
# ---------------------------
def detect_audio_type(row: pd.Series, filename: str) -> str:
    """Determine the file type (exhalation / cough / three_cough) based on the metadata row, otherwise return 'unknown'"""
    for t in ["exhalation", "cough", "three_cough"]:
        col = f"{t}_file_name"
        if col in row and pd.notna(row[col]) and str(row[col]) == filename:
            return t
    return "unknown"


def extract_pid_from_filename(fname: str) -> str:
    """Match the leading numbers in the filename as the patient ID"""
    m = re.match(r"^(\d+)", fname)
    return m.group(1) if m else ""


def llm_generate_descriptions(
        audio_dir: str,
        metadata_csv: str,
        thinker_model_path: str,
        device: str,
        batch_size: int,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        output_dir: str,
) -> Path:
    """
    Iterate through all .wav files under audio_dir, find the patient row by filename prefix,
    construct a prompt, and generate descriptions in batches.
    Output JSONL path: output_dir/audio_descriptions.jsonl
    """
    audio_dir = Path(audio_dir)
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "audio_descriptions.jsonl"

    # Read metadata
    df = pd.read_csv(metadata_csv)
    df["participant_identifier"] = df["participant_identifier"].astype(str)
    pid2row = {str(r["participant_identifier"]): r for _, r in df.iterrows()}

    # Collect tasks
    wav_files = sorted([p for p in audio_dir.glob("*.wav") if p.is_file()])
    tasks = []
    for p in wav_files:
        pid = extract_pid_from_filename(p.name)
        if not pid or pid not in pid2row:
            print(
                f"[WARN] Could not parse patient ID for {p.name} or it was not found in the CSV. Skipping description generation.")
            continue
        row = pid2row[pid]
        a_type = detect_audio_type(row, p.name)
        prompt = build_prompt(row, p.name, a_type)
        tasks.append((p.name, row, prompt))

    if not tasks:
        print("[WARN] No files found for which descriptions can be generated.")
        # Still create an empty file to avoid errors in subsequent processes
        out_path.write_text("", encoding="utf-8")
        return out_path

    print(f"[LLM] Loading Thinker model locally from: {thinker_model_path} to {device}")
    tokenizer = AutoTokenizer.from_pretrained(thinker_model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        thinker_model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=device
    )
    model.eval()

    # Batch generation
    with open(out_path, "w", encoding="utf-8") as f_out:
        for i in tqdm(range(0, len(tasks), batch_size), desc="Generating descriptions"):
            batch = tasks[i:i + batch_size]
            prompts = [t[2] for t in batch]
            inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=4096).to(device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    pad_token_id=tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=temperature,
                    top_p=top_p,
                )
            gen = outputs[:, inputs["input_ids"].shape[1]:]
            texts = tokenizer.batch_decode(gen, skip_special_tokens=True)

            for (fname, row, _), resp in zip(batch, texts):
                desc = clean_llm_output(resp)
                covid_test = str(row.get("covid_test_result", "Unknown"))
                disease_label = "COVID-19" if covid_test == "Positive" else "Not detected"
                item = {"audio_filename": fname, "description": desc, "disease": disease_label}
                f_out.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"[LLM] Wrote to: {out_path}")
    return out_path


# ---------------------------
# Step 2: Longformer Inference
# ---------------------------
FIXED_AUDIO_STEPS = 496
DEFAULT_MAX_SEQ_LEN = 4096
DEFAULT_MAX_TEXT_TOKENS = 128


def find_best_checkpoint(ckpt_dir: str) -> str:
    import re as _re, glob as _glob
    pattern = os.path.join(ckpt_dir, "best_longformer_loss_*.pth")
    files = _glob.glob(pattern)
    if not files:
        raise FileNotFoundError(f"No 'best_longformer_loss_*.pth' found in {ckpt_dir}.")
    best_path, best_loss = None, float("inf")
    for f in files:
        m = _re.search(r"best_longformer_loss_(\d+\.\d+)_epoch_\d+\.pth$", os.path.basename(f))
        if not m:
            continue
        loss = float(m.group(1))
        if loss < best_loss:
            best_loss, best_path = loss, f
    if best_path is None:
        raise RuntimeError("Could not parse loss value from filename, please check the naming convention.")
    print(f"[CKPT] Using best checkpoint: {os.path.basename(best_path)} (loss={best_loss:.4f})")
    return best_path


class LongformerWithBEATsInfer:
    """Inference process wrapper"""

    def __init__(self, config: dict, checkpoint_path: str, idx_to_disease: dict, device: torch.device):
        """
        Make diagnoser_pipeline load the fine-tuned Longformer weights.
        Without loading `model_state_dict`, the model runs with (mostly) base weights and produces bad predictions.
        """
        self.cfg = config
        self.device = device

        # 1) Load training checkpoint and its saved client/config metadata
        pack = torch.load(checkpoint_path, map_location=device)
        client = pack.get("client_state", {})
        self.train_cfg = self.cfg

        # 2) Hyperparameters (must be consistent with training)
        self.model_name = (
            self.train_cfg.get("hyperparameters", {}).get("longformer", {}).get("model_name",
                                                                                "allenai/longformer-base-4096")
        )
        self.max_seq_len = (
            self.train_cfg.get("hyperparameters", {}).get("longformer", {}).get("max_sequence_length",
                                                                                DEFAULT_MAX_SEQ_LEN)
        )
        self.max_text_tokens = (
            self.train_cfg.get("hyperparameters", {}).get("longformer", {}).get("max_text_tokens",
                                                                                DEFAULT_MAX_TEXT_TOKENS)
        )
        self.beats_feat_dim = (
            self.train_cfg.get("hyperparameters", {}).get("longformer", {}).get("beats_feature_size", 768)
        )
        self.audio_global_stride = int(
            self.train_cfg.get("hyperparameters", {}).get("longformer", {}).get("audio_global_stride", 8)
        )

        # 3) Tokenizer (extend with task-specific special tokens)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)
        if self.tokenizer.pad_token_id is None:
            # Prefer EOS as pad if available; otherwise create a literal [PAD]
            if self.tokenizer.eos_token:
                self.tokenizer.add_special_tokens({"pad_token": self.tokenizer.eos_token})
            else:
                self.tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        self.tokenizer.padding_side = "right"

        base_special_tokens = ["[DESCRIPTION]"]
        audio_embed_tokens = [f"[AUDIO_EMBED_{i}]" for i in range(1, FIXED_AUDIO_STEPS + 1)]
        self.tokenizer.add_special_tokens({"additional_special_tokens": base_special_tokens + audio_embed_tokens})

        # Ensure these tokens will not be split by the tokenizer
        no_split = set(getattr(self.tokenizer, "unique_no_split_tokens", []))
        no_split.update(base_special_tokens + audio_embed_tokens)
        self.tokenizer.unique_no_split_tokens = list(no_split)

        self.desc_tok_id = self.tokenizer.convert_tokens_to_ids("[DESCRIPTION]")
        self.audio_embed_ids = self.tokenizer.convert_tokens_to_ids(audio_embed_tokens)
        self.audio_embed_start_id = self.tokenizer.convert_tokens_to_ids("[AUDIO_EMBED_1]")

        # 4) Model (critical fix: load the fine-tuned model_state_dict)
        num_labels = int(client.get("num_labels", len(idx_to_disease)))
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, num_labels=num_labels, problem_type="single_label_classification"
        )
        # Must resize after adding special tokens to the tokenizer
        self.model.resize_token_embeddings(len(self.tokenizer))

        # Projection from BEATs feature dim -> Longformer embedding dim
        hidden = self.model.get_input_embeddings().embedding_dim
        self.model.projection_layer = nn.Linear(self.beats_feat_dim, hidden)

        # Load fine-tuned backbone + classifier weights
        if "model_state_dict" in pack:
            missing, unexpected = self.model.load_state_dict(pack["model_state_dict"], strict=False)
            if missing:
                print(f"[CKPT] Missing keys: {len(missing)} "
                      f"(often benign due to tokenizer expansion / embedding size changes)")
            if unexpected:
                print(f"[CKPT] Unexpected keys: {len(unexpected)}")
        else:
            print(
                "[CKPT][WARN] 'model_state_dict' not found; falling back to base Longformer weights (expect poor performance).")

        # Load separately saved projection layer if present
        if "projection_layer_state_dict" in pack:
            try:
                self.model.projection_layer.load_state_dict(pack["projection_layer_state_dict"], strict=False)
            except Exception as e:
                print(f"[CKPT] Failed to load projection_layer_state_dict: {e}")

        self.model.to(self.device).eval()

        # 5) BEATs feature extractor
        beats_ckpt_path = self.train_cfg.get("paths", {}).get("beats_feature_extractor_checkpoint")
        if not beats_ckpt_path or not os.path.exists(beats_ckpt_path):
            raise FileNotFoundError(
                f"BEATs checkpoint not found: {beats_ckpt_path} "
                f"(set a valid path in config.yaml under paths.beats_feature_extractor_checkpoint)"
            )
        ckpt = torch.load(beats_ckpt_path, map_location="cpu")
        bcfg = BEATsConfig(ckpt["cfg"])
        self.beats = BEATs(bcfg)
        self.beats.load_state_dict(ckpt["model"])
        self.beats.eval().to(self.device)

    @staticmethod
    def align_to_fixed_steps(x: torch.Tensor, steps: int = FIXED_AUDIO_STEPS, mode: str = "center"):
        B, T, D = x.shape
        if T == steps:
            return x
        if T > steps:
            if mode == "left":
                return x[:, :steps, :]
            if mode == "right":
                return x[:, -steps:, :]
            start = (T - steps) // 2
            return x[:, start:start + steps, :]
        pad_len = steps - T
        pad = x.new_zeros(B, pad_len, D)
        return torch.cat([x, pad], dim=1)

    def prepare_batch(self, waveforms: torch.Tensor, descriptions: List[str]):
        """Returns: (inputs_embeds, attention_mask, global_attention_mask)"""
        B = waveforms.size(0)
        with torch.no_grad():
            feats, _ = self.beats.extract_features(waveforms.to(self.device), padding_mask=None)
        feats = self.align_to_fixed_steps(feats, FIXED_AUDIO_STEPS)
        projected = self.model.projection_layer(feats)

        cls_id = self.tokenizer.cls_token_id
        sep_id = self.tokenizer.sep_token_id
        pad_id = self.tokenizer.pad_token_id
        overhead = 3
        desc_budget = max(0, self.max_seq_len - FIXED_AUDIO_STEPS - overhead)
        desc_budget = min(desc_budget, self.max_text_tokens)

        batch_ids = []
        for i in range(B):
            desc = descriptions[i] if isinstance(descriptions[i], str) else str(descriptions[i])
            desc_ids = self.tokenizer.encode(desc, add_special_tokens=False, truncation=True, max_length=desc_budget)
            seq = [cls_id, self.desc_tok_id] + desc_ids + self.audio_embed_ids + [sep_id]
            if len(seq) > self.max_seq_len:
                over = len(seq) - self.max_seq_len
                if over > 0 and len(desc_ids) > 0:
                    desc_ids = desc_ids[:max(0, len(desc_ids) - over)]
                    seq = [cls_id, self.desc_tok_id] + desc_ids + self.audio_embed_ids + [sep_id]
                if len(seq) > self.max_seq_len:
                    seq = seq[:self.max_seq_len]
            batch_ids.append(seq)

        max_len = max(len(s) for s in batch_ids)
        input_ids_list, attn_mask_list = [], []
        for seq in batch_ids:
            pad_len = max_len - len(seq)
            input_ids_list.append(seq + [pad_id] * pad_len)
            attn_mask_list.append([1] * len(seq) + [0] * pad_len)

        input_ids = torch.tensor(input_ids_list, dtype=torch.long, device=self.device)
        attn_mask = torch.tensor(attn_mask_list, dtype=torch.long, device=self.device)

        input_embeds = self.model.get_input_embeddings()(input_ids)
        for i in range(input_embeds.size(0)):
            ids = input_ids[i]
            pos_audio = (ids == self.audio_embed_start_id).nonzero(as_tuple=True)[0]
            if len(pos_audio) == 0:
                continue
            s = int(pos_audio[0].item())
            e = min(s + FIXED_AUDIO_STEPS, input_embeds.size(1))
            n = e - s
            if n > 0:
                input_embeds[i, s:e, :] = projected[i, :n, :]

        global_attention_mask = torch.zeros_like(attn_mask, dtype=torch.long)
        for i in range(input_ids.size(0)):
            ids = input_ids[i]
            pos_cls = (ids == cls_id).nonzero(as_tuple=True)[0]
            pos_desc = (ids == self.desc_tok_id).nonzero(as_tuple=True)[0]
            if len(pos_cls) > 0:
                global_attention_mask[i, int(pos_cls[0].item())] = 1
            if len(pos_desc) > 0:
                global_attention_mask[i, int(pos_desc[0].item())] = 1

        # Stride sampling for audio anchors
        stride = max(1, int(self.audio_global_stride))
        Bk, L = attn_mask.shape
        for i in range(input_ids.size(0)):
            ids = input_ids[i]
            pos_audio = (ids == self.audio_embed_start_id).nonzero(as_tuple=True)[0]
            if len(pos_audio) == 0:
                continue
            audio_start = int(pos_audio[0].item())
            audio_end = min(audio_start + FIXED_AUDIO_STEPS, L)
            T = max(0, audio_end - audio_start)
            if T <= 0:
                continue
            idx = torch.arange(T, device=self.device).unsqueeze(0)
            pick = (idx % stride == 0).long()
            global_attention_mask[i, audio_start:audio_end] = pick.squeeze(0)

        return input_embeds, attn_mask, global_attention_mask

    def forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor,
                global_attention_mask: torch.Tensor):
        with torch.no_grad():
            out = self.model(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                global_attention_mask=global_attention_mask,
                labels=None
            )
        return out


# ---------------------------
# Main Process
# ---------------------------
def main():
    args = parse_args()
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    audio_dir = Path(args.audio_dir)
    audio_dir.mkdir(parents=True, exist_ok=True)

    # Ensure output_dir exists
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1) Generate descriptions (audio_descriptions.jsonl) and write to output_dir
    jsonl_path = llm_generate_descriptions(
        audio_dir=str(audio_dir),
        metadata_csv=args.metadata_csv,
        thinker_model_path=args.thinker_model_path,
        device=args.device,
        batch_size=args.batch_size,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_p=args.top_p,
        output_dir=str(output_dir),
    )

    # 2) Longformer inference and comparison with COVID results
    # 2.1 Load configuration & find the best checkpoint
    with open(args.config_yaml, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    target_sr = cfg.get("audio", {}).get("sample_rate", 16000)
    target_len = cfg.get("audio", {}).get("max_length", target_sr * 10)

    best_ckpt = find_best_checkpoint(args.ckpt_dir)
    pack = torch.load(best_ckpt, map_location="cpu")
    client = pack.get("client_state", {})
    idx_to_disease = client.get("label_mapping")
    if not idx_to_disease:
        raise RuntimeError("The checkpoint's client_state is missing label_mapping (idx->disease).")
    idx_to_disease = {int(k): v for k, v in idx_to_disease.items()}

    # 2.2 Load descriptions from jsonl
    desc_map: Dict[str, str] = {}
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            rec = json.loads(line)
            fn, desc = rec.get("audio_filename"), rec.get("description")
            if fn and isinstance(desc, str):
                desc_map[fn] = desc

    # 2.3 Metadata (to get ground truth COVID status)
    df = pd.read_csv(args.metadata_csv)
    df["participant_identifier"] = df["participant_identifier"].astype(str)
    pid2covid = {str(r["participant_identifier"]): str(r.get("covid_test_result", "Unknown")) for _, r in df.iterrows()}

    # 2.4 Model wrapper initialization
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    infer = LongformerWithBEATsInfer(cfg, best_ckpt, idx_to_disease, device)

    # 2.5 Perform inference file-by-file and compare with COVID results
    results: List[Tuple[str, str, str, bool]] = []
    wav_files = sorted([p for p in audio_dir.glob("*.wav") if p.is_file()])

    print("\n[Diagnoser] Starting file-by-file inference and comparison with COVID results:")
    for p in tqdm(wav_files, desc="Inferring"):
        fname = p.name
        if fname not in desc_map:
            print(f"[SKIP] {fname} not found in audio_descriptions.jsonl, skipping.")
            continue

        # Ground truth COVID
        pid = extract_pid_from_filename(fname)
        covid_gt = pid2covid.get(pid, "Unknown")

        # Prepare waveform
        wav, sr = safe_load_audio(str(p))
        if wav is None:
            print(f"[WARN] Failed to load: {fname}, skipping.")
            continue
        wav = resample_pad_or_trim(wav, sr, target_sr, target_len)
        if wav is None:
            print(f"[WARN] Resampling failed: {fname}, skipping.")
            continue

        # Longformer assembly + inference
        inputs_embeds, attn_mask, gmask = infer.prepare_batch(wav.to(device), [desc_map[fname]])
        out = infer.forward(inputs_embeds, attn_mask, gmask)
        pred_idx = int(torch.argmax(out.logits, dim=1).item())
        pred_label = idx_to_disease.get(pred_idx, f"IDX_{pred_idx}")

        # COVID binary comparison: predict if it is COVID-19
        pred_is_covid = (pred_label == "COVID-19")
        gt_is_covid = (covid_gt == "Positive")
        covid_match = (pred_is_covid == gt_is_covid)

        results.append((fname, covid_gt, pred_label, covid_match))
        print(
            f"[FILE] {fname} | Patient={pid} | GT COVID={covid_gt} | Predicted={pred_label} | Match(COVID)? {covid_match}")

    # 2.6 Export summary
    if results:
        out_csv = output_dir / "diagnoser_results.csv"
        pd.DataFrame(
            results,
            columns=["audio_filename", "covid_test_result(GT)", "predicted_disease", "covid_match"]
        ).to_csv(out_csv, index=False)
        print(f"\n[Done] Results have been written to: {out_csv}")
    else:
        print("\n[Done] No results to save.")


if __name__ == "__main__":
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    main()
